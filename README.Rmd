---
title: "Data-Related Jobs for ESM Graduates"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(tidytext)
library(dplyr)
library(knitr)
library(DT)
source("scraping.R")
source("word lists.R")
```

## Purpose

The purpose of this project is to understand the skills and competencies of jobs potential ESM graduates may be qualified for.

## Data and Methodology

All data comes from administrative job ads [HigherEdJobs](http://www.higheredjobs.com) using the following searches:

* [Search 1](https://www.higheredjobs.com/search/advanced_action.cfm?JobCat=14&JobCat=212&JobCat=148&JobCat=27&JobCat=31&JobCat=43&PosType=1&PosType=2&InstType=1&InstType=2&InstType=3&Keyword=quantitative+OR+statistics+OR+measurement+OR+data&Remote=1&Remote=2&Region=&Submit=Search+Jobs)
* [Search 2](https://www.higheredjobs.com/search/advanced_action.cfm?PosType=1&PosType=2&InstType=1&InstType=2&InstType=3&Keyword=%22data+scientist%22+OR+%22data+engineer%22+OR+%22data+analyst%22&Remote=1&Remote=2&Region=&Submit=Search+Jobs) 

The initial plan was to use the *rvest* webscraper package to automatically crawl these searches and scrape these jobs. However, while testing, I realized only a limited amount of data could be downloaded before the webscraper was blocked as a bot. 

Due to webscraping limitations, jobs were manually downloaded from the searches. This had the added advantage of "human filtering" - removing jobs that were unrelated or clearly required discipline-specific degrees (e.g. business, biostats).

All downloaded jobs can be found in the **job-files** directory.

## Code

The content of the files is scraped using the *rvest* package. The code can be found in the **scraping.R** file. The code is then analyzed using *tidytext* and words from the **word lists.R** file.


## Job Ad Data {.tabset .tabset-fade}

``` {r data_setup, echo=FALSE}
tidy_jobs <- job_data %>% mutate(
  content = as.character(job_body))

tidy_jobs <- tidy_jobs %>% unnest_tokens(word, content) 
tidy_jobs <- tidy_jobs %>%
  distinct(word, job_file, .keep_all = TRUE) #remove duplicates per job

nrow(job_data) %>%
  kable(row.names = FALSE, col.names = "Number of Jobs Analyzed")
```
### Job Categories

``` {r job_categories, echo=FALSE}
job_data %>%
  filter(str_count(job_cat, ' ') > 0) %>%
  group_by(job_cat) %>% count() %>% 
  arrange(-n) %>% datatable(
    rownames = FALSE,
    colnames = c("Job Category", "Count"))
```

## Basic Findings {.tabset .tabset-fade}

### Programming/Software

``` {r langs, echo=FALSE}
job_langs <- tidy_jobs %>% inner_join(langs)
job_langs %>% count(word) %>% arrange(-n) %>%
  rename("Program/Language" = word) %>%
  kable(row.names = FALSE)
```

### Statistical Skills

``` {r stats, echo=FALSE}
job_stats <- tidy_jobs %>% inner_join(statistics)
job_stats %>% count(word) %>% arrange(-n) %>%
  rename("Statistics" = word) %>%
  kable(row.names = FALSE)
```


## Next Steps

To develop this project more, I plan to:

1. Add more jobs
2. Increase and refine already extant wordlists
3. Create a "skills" wordlist
4. Allow sorting by job type, location, etc.
  + Maybe a Shiny app?
